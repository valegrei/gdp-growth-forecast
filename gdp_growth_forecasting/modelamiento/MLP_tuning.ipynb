{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../libs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils import shift_join_data, print_hp\n",
    "from models import build_mlp\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras_tuner import HyperModel\n",
    "import datetime\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/wb_dataset_prep.csv')\n",
    "df = df.drop('country',axis=1)\n",
    "iso = df['iso'].unique()    #Codigos de paises\n",
    "df = df.set_index(['iso','year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividir Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_test = ['PER']\n",
    "iso_train = iso[(iso != iso_test[0])]\n",
    "target_col = ['rgdp_growth']\n",
    "features = df.columns[(df.columns!=target_col[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x_train = df.loc[iso_train][features].copy()\n",
    "df_y_train = df.loc[iso_train][target_col].copy()\n",
    "\n",
    "std_scaler_x_train = StandardScaler()\n",
    "std_scaler_y_train = StandardScaler()\n",
    "\n",
    "df_x_train.iloc[:,:] = std_scaler_x_train.fit_transform(df_x_train)\n",
    "df_y_train.iloc[:,:] = std_scaler_y_train.fit_transform(df_y_train)\n",
    "\n",
    "df_x_train.iloc[:,:] = np.clip(df_x_train,-5,5)\n",
    "df_y_train.iloc[:,:] = np.clip(df_y_train,-5,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPHyperModel(HyperModel):\n",
    "\n",
    "    def __init__(self, n_features_in, n_steps_out,countries ,metrics = ['mae'], name=None, tunable=True):\n",
    "        super().__init__(name=name, tunable=tunable)\n",
    "        self.n_features = n_features_in\n",
    "        self.n_steps_out = n_steps_out\n",
    "        self.countries = countries\n",
    "        self.metrics = metrics\n",
    "\n",
    "    def build(self, hp):\n",
    "        # Parametrizamos nro de capas, nro de nodos y ratio de aprendizaje\n",
    "        hp_time_steps = hp.Int('steps_in',4,10,step=1)\n",
    "        hp_capas = hp.Int('nro_capas',1,5)\n",
    "        hp_nodes = hp.Int('nro_nodos',32,356,step=16)\n",
    "        hp_ratio_aprendizaje = hp.Choice('ratio_aprendizaje', values=[1e-2, 1e-3, 1e-4])\n",
    "        hp_activation = hp.Choice('activacion', values=['relu','tanh'])\n",
    "\n",
    "        return build_mlp(\n",
    "            n_steps_in = hp_time_steps,\n",
    "            n_features = self.n_features,\n",
    "            n_steps_out = self.n_steps_out, \n",
    "            nodes = hp_nodes,\n",
    "            layers = hp_capas,\n",
    "            learning_rate = hp_ratio_aprendizaje,\n",
    "            activation = hp_activation,\n",
    "            metrics = self.metrics)\n",
    "\n",
    "    def fit(self, hp, model,x,y,**kwargs):\n",
    "        x_s,y_s = shift_join_data(x, y, self.countries, hp.get('steps_in'), self.n_steps_out)\n",
    "        n_batch = len(x_s)\n",
    "        return model.fit(x = x_s, y = y_s, batch_size = n_batch, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps_out = 3\n",
    "n_features = len(features)\n",
    "ajuste_path = 'tuning_output/'\n",
    "fecha_hora = datetime.datetime.now().strftime('%Y%m%d_%H%M')\n",
    "objective = 'val_mae'\n",
    "max_epochs = 1000\n",
    "hp_iters = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condicion de parada: 50 epocas despues del menor val_loss\n",
    "#rmse = RootMeanSquaredError(name='rmse')\n",
    "es = EarlyStopping(monitor='val_mae', mode='min', patience=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_path = 'MLP'+'_'+fecha_hora\n",
    "mlp_path_prj = mlp_path\n",
    "mlp_hypermodel = MLPHyperModel(n_features, n_steps_out, iso_train)\n",
    "\n",
    "mlp_tuner = kt.Hyperband(\n",
    "    mlp_hypermodel,\n",
    "    objective = objective,\n",
    "    max_epochs = max_epochs,\n",
    "    hyperband_iterations = hp_iters,\n",
    "    directory = ajuste_path,\n",
    "    project_name = mlp_path_prj,\n",
    "    overwrite=True)\n",
    "\n",
    "mlp_tuner.search(x = df_x_train, y = df_y_train, validation_split = 0.2, epochs = max_epochs\n",
    "    , verbose = 0, shuffle = False, callbacks = [es])\n",
    "# guardar mejor modelo\n",
    "best_mlp_hps = mlp_tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "print_hp(ajuste_path+mlp_path+'.txt',mlp_tuner)\n",
    "\n",
    "mlp_model = mlp_tuner.hypermodel.build(best_mlp_hps)\n",
    "mlp_model.save(ajuste_path+mlp_path+'.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('entornoGPU')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b5f45148ae3128c6dd6e07223b4f800e24d99a74d72c1203f234d906a82ee07"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
