{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../libs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils import shift_join_data, print_hp\n",
    "from models import build_lstm\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras_tuner import HyperModel\n",
    "import datetime\n",
    "import keras_tuner as kt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/wb_dataset_prep.csv')\n",
    "df = df.drop('country',axis=1)\n",
    "iso = df['iso'].unique()    #Codigos de paises\n",
    "df = df.set_index(['iso','year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividir Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_test = ['PER']\n",
    "iso_train = iso[(iso != iso_test[0])]\n",
    "target_col = ['rgdp_growth']\n",
    "features = df.columns[(df.columns!=target_col[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x_train = df.loc[iso_train][features].copy()\n",
    "df_y_train = df.loc[iso_train][target_col].copy()\n",
    "\n",
    "scaler_x_train = StandardScaler()\n",
    "scaler_y_train = StandardScaler()\n",
    "\n",
    "df_x_train.iloc[:,:] = scaler_x_train.fit_transform(df_x_train)\n",
    "df_y_train.iloc[:,:] = scaler_y_train.fit_transform(df_y_train)\n",
    "\n",
    "df_x_train.iloc[:,:] = np.clip(df_x_train,-5,5)\n",
    "df_y_train.iloc[:,:] = np.clip(df_y_train,-5,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMHyperModel(HyperModel):\n",
    "\n",
    "    def __init__(self, n_features_in, n_steps_out, countries, metrics = ['mae','mse','mape'], name = None, tunable = True):\n",
    "        super().__init__(name=name, tunable=tunable)\n",
    "        self.n_features = n_features_in\n",
    "        self.n_steps_out = n_steps_out\n",
    "        self.countries = countries\n",
    "        self.metrics = metrics\n",
    "\n",
    "    def build(self, hp):\n",
    "        # Parametrizamos nro de capas, nro de nodos y ratio de aprendizaje\n",
    "        hp_time_steps = hp.Int('steps_in',4,10,step=1)\n",
    "        hp_lstm_cells = hp.Int('lstm_cells',32,356,step=16)\n",
    "        hp_lstm_dropout = hp.Float('lstm_dropout',0.1,0.5,step=0.1)\n",
    "        hp_dense_layers = hp.Int('dense_layers',1,5,step=1)\n",
    "        hp_dense_nodes = hp.Int('dense_nodes',32,356,step=16)\n",
    "        hp_dense_dropout = hp.Float('dense_dropout',0.1,0.5,step=0.1)\n",
    "        hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "        hp_dense_activation = hp.Choice('dense_activation', values=['relu','tanh'])\n",
    "\n",
    "        return build_lstm(\n",
    "            n_steps_in = hp_time_steps,\n",
    "            n_features = self.n_features,\n",
    "            n_steps_out = self.n_steps_out, \n",
    "            lstm_cells = hp_lstm_cells,\n",
    "            lstm_dropout = hp_lstm_dropout,\n",
    "            dense_layers = hp_dense_layers,\n",
    "            dense_nodes = hp_dense_nodes,\n",
    "            dense_dropout = hp_dense_dropout,\n",
    "            learning_rate = hp_learning_rate,\n",
    "            dense_activation = hp_dense_activation,\n",
    "            metrics = self.metrics)\n",
    "\n",
    "    def fit(self, hp, model,x,y,**kwargs):\n",
    "        x_s,y_s = shift_join_data(x, y, self.countries, hp.get('steps_in'), self.n_steps_out)\n",
    "        mini_batch = 32\n",
    "        return model.fit(x = x_s, y = y_s, batch_size = mini_batch, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps_out = 3\n",
    "n_features = len(features)\n",
    "ajuste_path = os.path.normpath('G:/')\n",
    "fecha_hora = datetime.datetime.now().strftime('%Y%m%d_%H%M')\n",
    "objective = 'val_mse'\n",
    "max_epochs = 100\n",
    "hp_iters = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condicion de parada: 50 epocas despues del menor val_loss\n",
    "#rmse = RootMeanSquaredError(name='rmse')\n",
    "es = EarlyStopping(monitor='val_mse', mode='min', patience=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 254 Complete [00h 01m 12s]\n",
      "val_mse: 0.78281170129776\n",
      "\n",
      "Best val_mse So Far: 0.73088538646698\n",
      "Total elapsed time: 01h 03m 16s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "name_prj = 'LSTM'+'_'+fecha_hora\n",
    "lstm_hypermodel = LSTMHyperModel(n_features, n_steps_out, iso_train)\n",
    "\n",
    "lstm_tuner = kt.Hyperband(\n",
    "    lstm_hypermodel,\n",
    "    objective = objective,\n",
    "    max_epochs = max_epochs,\n",
    "    hyperband_iterations = hp_iters,\n",
    "    directory = ajuste_path,\n",
    "    project_name = name_prj,\n",
    "    overwrite=True)\n",
    "\n",
    "lstm_tuner.search(x = df_x_train, y = df_y_train, validation_split = 0.3, epochs = max_epochs\n",
    "    , verbose = 2, shuffle = False, callbacks = [es])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar mejor modelo\n",
    "output_path = 'ajustes/'\n",
    "best_lstm_hps = lstm_tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "print_hp(output_path+name_prj+'.txt',lstm_tuner)\n",
    "\n",
    "lstm_model = lstm_tuner.hypermodel.build(best_lstm_hps)\n",
    "lstm_model.save(output_path+name_prj+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from notifications import enviar_correo\n",
    "# enviar_correo(\"Ajuste de Parametros Finalizado!\",\"Se ha completado: {}\".format(name_prj))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('entornoGPU')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b5f45148ae3128c6dd6e07223b4f800e24d99a74d72c1203f234d906a82ee07"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
