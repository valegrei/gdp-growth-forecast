{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%xmode Verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../libs/\")\n",
    "sys.path.append(\"../../../deep-belief-network/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from utils import shift_join_data,rmse,print_line, plot_pred, flatten\n",
    "from dbn.models import SupervisedDBNRegression\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fecha_hora = datetime.datetime.now().strftime('%Y%m%d_%H%M')\n",
    "res_name = 'resultados/DBN_'+fecha_hora\n",
    "n_iter = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametros obtenidos por optimizacion\n",
    "rbm_layers = 2\n",
    "rbm_nodes = 112\n",
    "h_layers_structure = [rbm_nodes for _ in range(rbm_layers)]\n",
    "activation = 'tanh'\n",
    "dropout = 0.2\n",
    "rbm_learning_rate = 1e-4\n",
    "bp_learning_rate = 1e-2\n",
    "n_epochs = 20\n",
    "n_iter_backprop = 200\n",
    "mini_batch = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/wb_dataset_prep.csv')\n",
    "df = df.drop('country',axis=1)\n",
    "iso = df['iso'].unique()    #Codigos de paises\n",
    "df = df.set_index(['iso','year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividir Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_test = ['PER']\n",
    "iso_train = iso[(iso != iso_test[0])]\n",
    "target_col = ['rgdp_growth']\n",
    "features = df.columns[(df.columns!=target_col[0])]\n",
    "df_test = df.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Países de Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iso_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features de Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x_train = df.loc[iso_train][features].copy()\n",
    "df_y_train = df.loc[iso_train][target_col].copy()\n",
    "df_x_test = df_test.loc[iso_test][features].copy()\n",
    "df_y_test = df_test.loc[iso_test][target_col].copy()\n",
    "\n",
    "std_scaler_x_train = StandardScaler()\n",
    "std_scaler_y_train = StandardScaler()\n",
    "std_scaler_x_test = StandardScaler()\n",
    "std_scaler_y_test = StandardScaler()\n",
    "\n",
    "df_x_train.iloc[:,:] = std_scaler_x_train.fit_transform(df_x_train)\n",
    "df_y_train.iloc[:,:] = std_scaler_y_train.fit_transform(df_y_train)\n",
    "df_x_test.iloc[:,:] = std_scaler_x_test.fit_transform(df_x_test)\n",
    "df_y_test.iloc[:,:] = std_scaler_y_test.fit_transform(df_y_test)\n",
    "\n",
    "df_x_train.iloc[:,:] = np.clip(df_x_train,-3,3)\n",
    "df_y_train.iloc[:,:] = np.clip(df_y_train,-3,3)\n",
    "df_x_test.iloc[:,:] = np.clip(df_x_test,-3,3)\n",
    "df_y_test.iloc[:,:] = np.clip(df_y_test,-3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generar variables lag y horizonte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps_in = 10\n",
    "n_steps_out = 3\n",
    "n_features = len(features)\n",
    "\n",
    "x_train, y_train = shift_join_data(df_x_train,df_y_train,iso_train,n_steps_in,n_steps_out)\n",
    "x_test, y_test = shift_join_data(df_x_test,df_y_test,iso_test,n_steps_in,n_steps_out)\n",
    "x_test, y_test = x_test[- int(len(x_test)*0.2):], y_test[- int(len(y_test)*0.2):]\n",
    "x_train = flatten(x_train)\n",
    "x_test = flatten(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento y evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar_dbn(x_train : np.ndarray, y_train : np.ndarray, x_test : np.ndarray,\n",
    "        y_test : np.ndarray, n_iter : int, scaler : StandardScaler):\n",
    "    n_splits = 5    # Nro de K Folds para CV\n",
    "    res_path = res_name+'.csv'\n",
    "    tscv = TimeSeriesSplit(n_splits = n_splits)\n",
    "    print_line(\"rmse\\n\",res_path)\n",
    "    # Inicio\n",
    "    gl_rmse = list()\n",
    "    gl_models = list()\n",
    "    nro = 0\n",
    "    print('Inicio de evaluacion:')\n",
    "    for i in range(n_iter):\n",
    "        val_rmse = list()\n",
    "        for train_idx, _ in tscv.split(x_train):\n",
    "            # CV split\n",
    "            x_t, y_t = x_train[train_idx], y_train[train_idx]\n",
    "            # Entrenamiento\n",
    "            model = SupervisedDBNRegression(\n",
    "                        hidden_layers_structure = h_layers_structure,\n",
    "                        learning_rate_rbm = rbm_learning_rate,\n",
    "                        learning_rate = bp_learning_rate,\n",
    "                        n_epochs_rbm = n_epochs,\n",
    "                        n_iter_backprop = n_iter_backprop,\n",
    "                        batch_size = mini_batch,\n",
    "                        activation_function = activation,\n",
    "                        dropout_p = dropout,\n",
    "                        verbose = False)\n",
    "            model.fit(x_t, y_t)\n",
    "            # Prediccion\n",
    "            y_pred = model.predict(x_test)\n",
    "            # Denormalizando\n",
    "            dn_y_test = scaler.inverse_transform(y_test)\n",
    "            dn_y_pred = scaler.inverse_transform(y_pred)\n",
    "            # Evaluacion\n",
    "            val_rmse.append(rmse(dn_y_test, dn_y_pred))\n",
    "            gl_rmse.append(rmse(dn_y_test, dn_y_pred))\n",
    "            model.save(res_name+'_'+str(nro)+'.pickle')\n",
    "            gl_models.append(model)\n",
    "            nro += 1\n",
    "        # Promedios\n",
    "        mean_rmse = np.mean(val_rmse)\n",
    "        print_line('{}\\n'.format(mean_rmse),res_path)\n",
    "        print('Iter: {}/{} completado.'.format(i+1,n_iter))\n",
    "    print('Fin de evaluacion.')\n",
    "    mejor_modelo = gl_models[np.argmin(gl_rmse)]\n",
    "    print('Mejor modelo: {}'.format(mejor_modelo))\n",
    "    return mejor_modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluar\n",
    "mejor_modelo = evaluar_dbn(x_train, y_train, x_test, y_test,n_iter,std_scaler_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.read_csv(res_name+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.plot.box()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mejor_modelo.predict(x_test)\n",
    "dn_y_test = std_scaler_y_test.inverse_transform(y_test)\n",
    "dn_y_pred = std_scaler_y_test.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pred(dn_y_test, dn_y_pred,2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notifications import enviar_correo\n",
    "enviar_correo(\"Evaluacion Finalizado!\",\"Se ha completado: {}\".format(res_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e6822839c80bb80c42f7f9e096efdd447a89633a8e8a553b5cfb2012f3a4eafe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
